return (x - min(x)) / range
}
cancer_norm <- as.data.frame(lapply(cancer[2:n], normalize))
normalize(cancer$radius_mean
.
summary(cancer_norm$radius_mean)
summary(cancer_norm$area_mean)
normalize <- function(x) {
range <- max(x) - min(x)
return ((x - min(x)) / range)
}
summary(cancer_norm$area_mean)
cancer_norm <- as.data.frame(lapply(cancer[2:31], normalize))
summary(cancer_norm$area_mean)
cancer_train <- cancer_norm[1:469,]
cancer_test <- cancer_norm[470:569,]
View(cancer_test)
View(cancer)
cancer_train_label <- cancer[1:469, 1]
cancer_test_label <- cancer[470:569, 1]
View(cancer_test_label)
cancer_test_label
install.packages("class")
library(class)
cancer_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 3)
cancer_pred
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 21)
install.packages("gmodels")
library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_z <- as.data.frame(scale(cancer[-1]))
View(cancer_z)
summary(cancer_z)
var(cancer_z$radius_mean)
#split into training set and test set
cancer_train <- cancer_z[1:469,]
cancer_test <- cancer_z[470:569,]
#build label vectors for training and test set
cancer_train_label <- cancer[1:469, 1]
cancer_test_label <- cancer[470:569, 1]
#install package class which performs some classification functions
install.packages("class")
library(class)
#implement knn
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 21)
#evaluating model performance
install.packages("gmodels")
library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
install.packages("class")
install.packages("gmodels")
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 5)
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 1)
#evaluating model performance
#install.packages("gmodels")
#library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 3)
#evaluating model performance
#install.packages("gmodels")
#library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 5)
#evaluating model performance
#install.packages("gmodels")
#library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 7)
#evaluating model performance
#install.packages("gmodels")
#library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 10)
#evaluating model performance
#install.packages("gmodels")
#library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 11)
#evaluating model performance
#install.packages("gmodels")
#library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 12)
#evaluating model performance
#install.packages("gmodels")
#library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
cancer_test_pred <- knn(train = cancer_train, test = cancer_test,
cl = cancer_train_label, 15)
#evaluating model performance
#install.packages("gmodels")
#library("gmodels")
CrossTable(x = cancer_test_label, y = cancer_test_pred, prop.chisq = FALSE)
sms_raw <- read.csv("C:/Users/Administrator/Desktop/projects/machin-learning-brett-lantz/sms_spam.csv", stringsAsFactors = FALSE)
View(sms_raw)
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
install.packages("tm")
library("tm")
sms_corpus <- Corpus(VectorSource(sms_raw$text))
#sms spam classification
#Clean de variable environment
rm(list = ls(all = TRUE))
sms_raw <- read.csv("C:/Users/Administrator/Desktop/projects/machin-learning-brett-lantz/sms_spam.csv", stringsAsFactors = FALSE)
#get data frame structure
str(sms_raw)
#factor de class feature
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
#install text mining package
install.packages("tm")
library("tm")
#create a collection of text documents(corpus)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
install.packages("tm")
sms_corpus
View(sms_corpus)
print(sms_corpus)
inspect(sms_corpus[1:5])
inspect(sms_corpus[1:3])
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
stopwords()
corpus_clean <- tm_map(corpus_clean, stopwords())
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
#delete numbers
corpus_clean <- tm_map(corpus_clean, removeNumbers)
#delete stopwords e.g. and, i'd, why, to, not, some, so, than etc
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
#remove punctuation
corpus_clean <- tm_map(corpus_clean, removePunctuation)
#remove multiple blank spaces
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
#create a sparse matrix - it tokenizes words of sms
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_raw <- read.csv("C:/Users/Administrator/Desktop/projects/machin-learning-brett-lantz/sms_spam.csv", stringsAsFactors = FALSE)
sms_raw$type <- factor(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
#convert all text to lowercase
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
#delete numbers
corpus_clean <- tm_map(corpus_clean, removeNumbers)
#delete stopwords e.g. and, i'd, why, to, not, some, so, than etc
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
#remove punctuation
corpus_clean <- tm_map(corpus_clean, removePunctuation)
#remove multiple blank spaces
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
#create a sparse matrix - it tokenizes words of sms
sms_dtm <- DocumentTermMatrix(corpus_clean)
#the raw data
sms_raw_train <- sms_raw[1:4169,]
sms_raw_test <- sms_raw[4170:5574,]
#the doc term matrix
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5574,]
sms_corpus_train <- corpus_clean[1:4169,]
sms_corpus_test <- corpus_clean[4170:5574,]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test <- corpus_clean[4170:5574]
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
install.packages("wordcloud")
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
wordcloud(sms_corpus_train, min.freq = 40, random.order = TRUE)
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
spam <- subset(sms_corpus_train, type == "spam")
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
warnings()
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
spam <- subset(sms_corpus_train, type == "spam")
spam <- subset(sms_raw_train, type == "spam")
ham <- subset(sms_raw_train, type == "ham")
View(spam)
wordcloud(spam$text, max.words = 40)
wordcloud(spam$text)
wordcloud(spam$text, max.words = 20)
wordcloud(ham$text, max.words = 40)
wordcloud(ham$text, max.words = 20)
wordcloud(spam$text, max.words = 20)
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40)
findFreqTerms(sms_dtm_train, 5)
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
?dictionary
??dictionary
sms_dict <- findFreqTerms(sms_dtm_train, 5)
sms_dict
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
sms_test
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0,1), labels = c(""No"", ""Yes""))
return (x)
}
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0,1), labels = c("No", "Yes))
return (x)
}
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0,1), labels = c("No", "Yes"))
return (x)
}
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0,1), labels = c("No", "Yes"))
return (x)
}
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_test, MARGIN = 2, convert_counts)
install.packages("e1071")
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_classifier
sms_test_pred <- predict(sms_classifier, sms_test)
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE,
prop.t = FALSE, dnn = c('predicted', 'actual'))
sms_classifier2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_raw_test$type, prop.chisq = FALSE,
prop.t = FALSE, dnn = c('predicted', actual))
CrossTable(sms_test_pred2, sms_raw_test$type, prop.chisq = FALSE,
prop.t = FALSE, dnn = c('predicted', 'actual'))
sms_classifier2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 2)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_raw_test$type, prop.chisq = FALSE,
prop.t = FALSE, dnn = c('predicted', 'actual'))
sms_classifier2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_raw_test$type, prop.chisq = FALSE,
prop.t = FALSE, dnn = c('predicted', 'actual'))
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
#predict
sms_test_pred <- predict(sms_classifier, sms_test)
#evaluate model
CrossTable(sms_test_pred, sms_raw_test$type, prop.chisq = FALSE,
prop.t = FALSE, dnn = c('predicted', 'actual'))
#model with laplace = 2 (35 examples errouneosly classifed)
sms_classifier2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 2)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_raw_test$type, prop.chisq = FALSE,
prop.t = FALSE, dnn = c('predicted', 'actual'))
stopwords()
install.packages("tm")
library("tm")
stopwords()
stopwords(kind = "sp")
#sms spam classification
#Clean de variable environment
rm(list = ls(all = TRUE))
#Get the data from CSV
sms_raw <- read.csv("C:/Users/Administrator/Desktop/projects/machin-learning-brett-lantz/sms_spam.csv", stringsAsFactors = FALSE)
#get data frame structure
str(sms_raw)
#factor de class feature
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
#install text mining package
install.packages("tm")
library("tm")
#create a collection of text documents(corpus)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
#convert all text to lowercase
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
#delete numbers
corpus_clean <- tm_map(corpus_clean, removeNumbers)
#delete stopwords e.g. and, i'd, why, to, not, some, so, than etc
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
#remove punctuation
corpus_clean <- tm_map(corpus_clean, removePunctuation)
#remove multiple blank spaces
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
#create a sparse matrix - it tokenizes words of sms
sms_dtm <- DocumentTermMatrix(corpus_clean)
#split training set and test set
#the raw data
sms_raw_train <- sms_raw[1:4169,]
sms_raw_test <- sms_raw[4170:5574,]
#the doc term matrix
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5574,]
#the corpus clean data
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test <- corpus_clean[4170:5574]
#check if subsets are representative
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
#visualize
install.packages("wordcloud")
library(wordcloud)
#build a wordcloud - most frequent words are larger than others
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
#build wordcloud but as spam and ham separeted datasets
spam <- subset(sms_raw_train, type == "spam")
ham <- subset(sms_raw_train, type == "ham")
#rebuild wordclouds for spam and ham separately
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40)
#find and save words that appear at least 5 times
sms_dict <- findFreqTerms(sms_dtm_train, 5)
#limit our datasets to only use those terms we've just found
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
#factor the document term matrix
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0,1), labels = c("No", "Yes"))
return (x)
}
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_test, MARGIN = 2, convert_counts)
#install packages naive bayes
install.packages("e1071")
library(e1071)
#build model (laplace = 0 , 34 examples errouneosly classifed)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
#predict
sms_test_pred <- predict(sms_classifier, sms_test)
sms_raw <- read.csv("C:/Users/Administrator/Desktop/projects/machin-learning-brett-lantz/sms_spam.csv", stringsAsFactors = FALSE)
sms_raw <- read.csv("C:/Users/Administrator/Desktop/projects/machin-learning-brett-lantz/sms_spam.csv", stringsAsFactors = FALSE)
sms_raw <- read.csv("C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_spam.csv", stringsAsFactors = FALSE)
rm(list = ls(all = TRUE))
#Get the data from CSV
sms_raw <- read.csv("C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_spam.csv", stringsAsFactors = FALSE)
#get data frame structure
str(sms_raw)
#factor de class feature
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, PlainTextDocument)
#delete numbers
corpus_clean <- tm_map(corpus_clean, removeNumbers)
#delete stopwords e.g. and, i'd, why, to, not, some, so, than etc
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
#remove punctuation
corpus_clean <- tm_map(corpus_clean, removePunctuation)
#remove multiple blank spaces
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
#create a sparse matrix - it tokenizes words of sms
sms_dtm <- DocumentTermMatrix(corpus_clean)
#split training set and test set
#the raw data
sms_raw_train <- sms_raw[1:4169,]
sms_raw_test <- sms_raw[4170:5574,]
#the doc term matrix
sms_dtm_train <- sms_dtm[1:4169,]
sms_dtm_test <- sms_dtm[4170:5574,]
#the corpus clean data
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test <- corpus_clean[4170:5574]
#check if subsets are representative
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
wordcloud(sms_corpus_train, min.freq = 40, random.order = FALSE)
#build wordcloud but as spam and ham separeted datasets
spam <- subset(sms_raw_train, type == "spam")
ham <- subset(sms_raw_train, type == "ham")
#rebuild wordclouds for spam and ham separately
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40)
sms_dict <- findFreqTerms(sms_dtm_train, 5)
#limit our datasets to only use those terms we've just found
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0,1), labels = c("No", "Yes"))
return (x)
}
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test <- apply(sms_test, MARGIN = 2, convert_counts)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
#predict
sms_test_pred <- predict(sms_classifier, sms_test)
View(sms_test_pred)
sms_test_pred
sms_result$actual_type <- sms_raw_test$type
sms_result$actual_type <- as.data.frame(sms_raw_test$type)
sms_raw_test$type
sms_result <- data.frame(sms_raw_test$type, sms_test_pred)
View(sms_result)
sms_result <- data.frame(sms_raw_test$type, sms_test_pred, colnames = c("actual_type", "predict_type"))
sms_test_pred
sms_classifier
head(sms_classifier)
?predict
sms_prob <- predict(sms_classifier, sms_test, type = "raw")
sms_prob
sms_result$prob_spam <- sms_prob
options(scipen=999)
sms_result$prob_spam <- sms_prob
sms_prob
head(sms_prob)
str(sms_prob)
sms_prob
head(sms_prob)
options(scipen=999)
head(sms_prob)
options(scipen=0)
head(sms_prob)
sms_result$prob_spam <- sms_prob[,2]
sms_prob[,2]
head(sms_result)
colnames(sms_result) <- c("actual_type", "predict_type", "spam_prob")
colnames(sms_result) <- c("actual_type", "predict_type", "prob_spam")
View(sms_result)
write.csv("sms_results", sep = ",")
write.csv(sms_result, "sms_results", sep = ",")
?write.csv
write.csv(sms_result, file = "sms_results", sep = ",")
write.csv(sms_result, file = "sms_results")
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results")
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results", sep = ",")
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv")
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quotes = FALSE)
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quote = FALSE)
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quote = FALSE)
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quote = FALSE, col.names = FALSE)
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quote = FALSE, col.names = FALSE)
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quote = FALSE, colnames = FALSE)
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quote = FALSE, col.names = FALSE)
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quote = FALSE, col.names = NA)
write.csv(sms_result, file = "C:/Users/Administrator/Desktop/projects/machine-learning-brett-lantz/sms_results.csv", quote = FALSE, row.names = FALSE)
rm(list = ls(all = TRUE))
#Get the data from CSV
rm(list = ls(all = TRUE))
cancer <- read.csv("http://pastebin.com/raw/WPkRNugK", header = TRUE, sep = ",", stringsAsFactors = FALSE)
View(cancer)
cancer <- cancer[-1]
table(cancer$diagnosis)
cancer$diagnosis <- factor(cancer$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
round(prop.table(table(cancer$diagnosis)) * 100, digit = 1)
#check summary of a few features
summary(cancer[c("radius_mean", "area_mean", "smoothness_mean")])
str(cancer)
rm(list = ls(all = TRUE))
#Get the data from CSV
cancer <- read.csv("http://pastebin.com/raw/WPkRNugK", header = TRUE, sep = ",", stringsAsFactors = FALSE)
#exclude id feature / not important
cancer <- cancer[-1]
iris
View(iris)
View(iris)
summary(iris)
str(iris)
plot(iris$Petal.Length, iris$Petal.Width, pch=21, bg=c("red","green3","blue")[unclass(iris$Species)])
iris_clusters <- kmeans(iris, 3)
iris$Species <- NULL
View(iris)
iris_clusters <- kmeans(iris, 3)
iris_clusters
unclass(iris_clusters)
plot(iris)
View(iris)
iris_clusters$cluster
iris$clase <- iris_clusters$cluster
View(iris)
iris
plot(iris$Petal.Length, iris$Petal.Width, pch=21, bg=c("red","green3","blue")[unclass(iris$clase)])
plot(iris$Sepal.Length, iris$Sepal.Width, pch=21, bg=c("red","green3","blue")[unclass(iris$clase)])
iris_clusters <- kmeans(iris, 2)
iris$clase <- iris_clusters$cluster
plot(iris$Sepal.Length, iris$Sepal.Width, pch=21, bg=c("red","green3","blue")[unclass(iris$clase)])
plot(iris$Sepal.Length, iris$Petal.Width, pch=21, bg=c("red","green3","blue")[unclass(iris$clase)])
plot(iris$Petal.Length, iris$Petal.Width, pch=21, bg=c("red","green3","blue")[unclass(iris$clase)])
pairs(iris[1:4], pch = 21, bg = c("red", "green3", "blue")[unclass(iris$clase)])
iris_clusters <- kmeans(iris, 3)
iris$clase <- iris_clusters$cluster
pairs(iris[1:4], pch = 21, bg = c("red", "green3", "blue")[unclass(iris$clase)])
iris
iris$Species <- NULL
iris_clusters <- kmeans(iris, 2)
iris$clase <- iris_clusters$cluster
pairs(iris[1:4], pch = 21, bg = c("red", "green3", "blue")[unclass(iris$clase)])
iris_clusters <- kmeans(iris, 3)
iris$clase <- iris_clusters$cluster
pairs(iris[1:4], pch = 21, bg = c("red", "green3", "blue")[unclass(iris$clase)])
iris_clusters <- kmeans(iris, 4)
iris$clase <- iris_clusters$cluster
pairs(iris[1:4], pch = 21, bg = c("red", "green3", "blue")[unclass(iris$clase)])
